{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649caa3b-6a25-4ed5-bb5c-da9deb465d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.llms import GPT4All \n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad550383-d8e3-410e-84d0-07be58cc5f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./data/2402.03367.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "segments = loader.load_and_split(text_splitter=text_splitter) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5dca2b0-5711-4a07-b827-506c4156a979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b07a359-7c1b-407a-8c6d-abcf73650699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='RAG-F USION :ANEWTAKE ON RETRIEVAL -AUGMENTED\\nGENERATION∗\\nZackary Rackauckas\\nInfineon Technologies\\nSan Jose, CA\\nzackary.rackauckas@infineon.com\\nABSTRACT\\nInfineon has identified a need for engineers, account managers, and customers to rapidly obtain\\nproduct information. This problem is traditionally addressed with retrieval-augmented generation\\n(RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method.', metadata={'source': './data/2402.03367.pdf', 'page': 0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c717f31-ab8d-4b6e-80b8-53f7582ade47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(documents=segments, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2876c6-1328-4e40-9d82-b61e142146d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Explain RAG-fusion and how it works.\"\n",
    "docs = vectorstore.similarity_search(question, k=5)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "092d0c46-c612-4b51-878c-11308fd9a24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">--page_content--<\n",
      "The largest challenge of RAG-Fusion is the slow answer time from receiving the query to outputting the answer. I\n",
      "compared the runtime to our traditional RAG chatbot by performing ten back-to-back runs with the same query. I then\n",
      "subtracted the time when the query was received from the time the output was given to determine the time it took for\n",
      "that run. Back-to-back runs should control for APIs having different response times at different times of the day.\n",
      ">--metadata--<\n",
      "{'page': 5, 'source': './data/2402.03367.pdf'}\n",
      ">--type--<\n",
      "Document\n",
      "\n",
      ">--page_content--<\n",
      "RAG-Fusion: a New Take on Retrieval-Augmented Generation\n",
      "Figure 1: Diagram illustrating the high level process of RAG-Fusion starting with the original query ”IM72D128 IP\n",
      "Rating”\n",
      "Infineon Chatbot for Engineers\n",
      "There are three potential use cases of the Infineon RAG-Fusion chatbot: providing technical information to engineers,\n",
      "providing sales-oriented information to account managers, and providing customer-facing information to customers.\n",
      ">--metadata--<\n",
      "{'page': 2, 'source': './data/2402.03367.pdf'}\n",
      ">--type--<\n",
      "Document\n",
      "\n",
      ">--page_content--<\n",
      "algorithm yields RAG-Fusion, a novel RAG-based chatbot model popularized by Adrian H. Raudaschl. [9]\n",
      "The chatbot I developed in this paper was initially a traditional RAG bot developed to be used by automotive field\n",
      "engineers. But I found that there was a need for customers and distributors to use this chatbot as well from questions\n",
      "asked online in our developer community. [ 10] Accordingly, I changed the model of the chatbot from RAG to RAG-\n",
      ">--metadata--<\n",
      "{'page': 1, 'source': './data/2402.03367.pdf'}\n",
      ">--type--<\n",
      "Document\n",
      "\n",
      ">--page_content--<\n",
      "While RAG-Fusion increases answer quality, it comes with challenges such as a longer runtime than other models\n",
      "due to a more complex call to the LLM with multiple queries and more documents, answers going off track from the\n",
      "original query due to irrelevant queries generated by the first LLM call, and the occasional need for appropriate prompt\n",
      "engineering to generate the desired outcome.\n",
      ">--metadata--<\n",
      "{'page': 6, 'source': './data/2402.03367.pdf'}\n",
      ">--type--<\n",
      "Document\n",
      "\n",
      ">--page_content--<\n",
      "compared to RAG can be mostly attributed to the second API call to the large language model. Even in long queries of\n",
      "70 or more words, the call to generate multiple queries never took more than 5 seconds. The model then ran through\n",
      "document retrieval and reciprocal rank fusion almost instantly until it stopped for the second API call for several\n",
      "seconds. The second API call’s complexity is amplified by the volume and diversity of the input in the form of multiple\n",
      ">--metadata--<\n",
      "{'page': 5, 'source': './data/2402.03367.pdf'}\n",
      ">--type--<\n",
      "Document\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    for k, v in dict(docs[i]).items():\n",
    "        print(f'>--{k}--<')\n",
    "        print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c104eef-1009-4ec3-af6b-dc373286c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4all = GPT4All(\n",
    "    model=\"./mistral-7b-openorca.Q4_0.gguf\",\n",
    "    max_tokens=2048,\n",
    "    temp=1, # 1) exp(x_i/T) / sum(exp(x_j/T))\n",
    "    # top_k=100, # 2) after temp, sort then select top k, normalize\n",
    "    # top_p=.5, # 3) after top_k, select until cum prob is reached, normalize\n",
    "    repeat_penalty=1.18,\n",
    "    repeat_last_n=64,\n",
    "    n_batch=8,\n",
    "    n_predict=None,\n",
    "    streaming=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1c01e3-e6ca-490e-85db-571b165fc0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' RAG-fusion, also known as Recombination Activating Gene (RAG) fusion, is a technique used in gene therapy to introduce specific genetic modifications into the DNA of cells. This process involves the use of two types of enzymes called recombinases and integrases that are derived from bacteriophages or other sources.\\n\\nThe RAG-fusion process works through several steps:\\n\\n1. Isolation of target cells: The first step is to isolate the specific type of cell that needs genetic modification, such as immune system cells like T-cells and B-cells. This can be done using various methods, including magnetic activated cell sorting (MACS) or fluorescence-activated cell sorting (FACS).\\n\\n2. Introduction of RAG enzymes: The next step is to introduce the recombinase and integrase enzymes into the cells. This can be done using viral vectors, such as adenoviruses or lentiviruses, which are engineered to carry the desired genetic material safely and efficiently.\\n\\n3. Expression of RAG enzymes: Once inside the cell, the recombinase and integrase enzymes express themselves and start working on the DNA. They recognize specific sequences in the target cells\\' genome called \"targeting sites\" or \"recombination signal sequences.\"\\n\\n4. Cutting and joining of DNA: The RAG enzymes cut the DNA at these recognition sites, creating a gap or break in the DNA strand. Then, they join the broken ends to another piece of DNA that has been introduced by the viral vector. This new DNA can be a gene of interest or a modified version of an existing gene.\\n\\n5. Integration and expression: The newly joined DNA is then integrated into the host cell\\'s genome using the integrase enzyme, which ensures that it remains stably expressed over time. Once integrated, the cells start expressing the new genetic material, leading to a desired phenotypic change or therapeutic effect.\\n\\nIn summary, RAG-fusion is a technique used in gene therapy for modifying cellular DNA by introducing specific genetic modifications through the use of recombinase and integrase enzymes. This process involves cutting and joining DNA strands at specific recognition sites, followed by integration into the host genome and expression of the new genetic material.')]], llm_output=None, run=[RunInfo(run_id=UUID('454b3e18-1eba-4125-a2e6-b4af38a87e2b'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without RAG\n",
    "gpt4all.generate([f\"You are an assistant for question-answering tasks.\\nQuestion: {question}\\nAnswer:\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f46049b-7e38-4486-8901-685bdf3166e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    ") # Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | gpt4all\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46231f2a-9094-4972-9d83-55d6867f1ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" RAG-Fusion is a type of chatbot model that combines Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs). It works by first generating multiple queries based on the original user query, and then calling an LLM to retrieve relevant documents. The retrieved information is used to generate an answer tailored to the user's question. This approach increases answer quality but may result in longer runtimes due to its more complex process compared to traditional RAG chatbots.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03c013fd-e37d-42be-bded-61325057bec9",
   "metadata": {},
   "source": [
    "How to Improve Quality\n",
    "\n",
    "easy\n",
    "- chunking parameters\n",
    "- model parameters\n",
    "- prompt engineering\n",
    "- two stage search (pull whole paragraph)\n",
    "\n",
    "medium\n",
    "- rag fusion\n",
    "- finetuning generator (may mess up model weights if done incorrectly)\n",
    "\n",
    "hard\n",
    "- finetuning retriever (requires labeled q-a data)\n",
    "\n",
    "Next Steps\n",
    "- deployment\n",
    "- add/remove docs from db\n",
    "- add source / page"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
